{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Training Single_MLP_fewshot\n",
      "==============================\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "Initial Epoch 0 | Test Acc: 6.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:00<00:00, 119.46batch/s, loss=1.9150, acc=27.83%, FLOPs=16.88G]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Test Acc: 53.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:00<00:00, 128.06batch/s, loss=1.5557, acc=59.50%, FLOPs=33.75G]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Test Acc: 70.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:00<00:00, 130.12batch/s, loss=1.3587, acc=72.33%, FLOPs=50.63G]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Test Acc: 76.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:00<00:00, 117.53batch/s, loss=1.1899, acc=76.67%, FLOPs=67.50G]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Test Acc: 79.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:00<00:00, 131.98batch/s, loss=1.0971, acc=77.75%, FLOPs=84.38G]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Test Acc: 81.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:00<00:00, 130.35batch/s, loss=0.9479, acc=80.83%, FLOPs=101.26G]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Test Acc: 83.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:00<00:00, 130.27batch/s, loss=0.8016, acc=83.17%, FLOPs=118.13G]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Test Acc: 84.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:00<00:00, 132.56batch/s, loss=0.9278, acc=83.83%, FLOPs=135.01G]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Test Acc: 85.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:00<00:00, 119.24batch/s, loss=0.7875, acc=83.92%, FLOPs=151.88G]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Test Acc: 85.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:00<00:00, 133.11batch/s, loss=0.7642, acc=83.17%, FLOPs=168.76G]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Test Acc: 86.34%\n"
     ]
    }
   ],
   "source": [
    "# File: train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from thop import profile\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from net import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "signed = True\n",
    "fewshot = True\n",
    "sample = False\n",
    "\n",
    "experiments = {\n",
    "    # \"Static_BaseRNN\": {\"type\": \"basernn\", \"trainable\": False, \"pruning\": None, \"optim\": \"adam\"},\n",
    "    # \"Static_BaseRNN_random\": {\"type\": \"basernn\", \"trainable\": False, \"pruning\": None, \"optim\": \"adam\", \"init\": \"random\"},\n",
    "    # \"Static_BaseRNN_RandSparse\": {\"type\": \"basernn\", \"trainable\": False, \"pruning\": None, \"optim\": \"adam\", \"init\": \"randsparse\"},\n",
    "    # \"Static_BaseRNN_RandStructure\":{\"type\": \"basernn\", \"trainable\": False, \"pruning\": None, \"optim\": \"adam\", \"init\": \"randstructure\"},\n",
    "    # \"Learnable_BaseRNN\": {\"type\": \"basernn\", \"trainable\": True, \"pruning\": None, \"optim\": \"adam\"},\n",
    "    \"Learnable_DrosophilaRNN\": {\"type\": \"drosophilarnn\", \"trainable\": True, \"pruning\": None, \"optim\": \"adam\"},\n",
    "    # \"CWS_Droso\": {\"type\": \"cwsrnn\", \"train_W\": True, \"train_C\": False, \"pruning\": None, \"optim\": \"adam\"},\n",
    "    # \"CWS_TrainC_pruning\": {\"type\": \"cwsrnn\", \"train_W\": True, \"train_C\": True, \"pruning\": \"drosophila\", \"optim\": \"adam\"},\n",
    "    # \"CWS_FixedC_random\": {\"type\": \"cwsrnn\", \"train_W\": True, \"train_C\": False, \"pruning\": None, \"init\": \"random\", \"optim\": \"adam\"},\n",
    "    # \"Static_CNN_RNN\": {\"type\": \"cnnrnn\", \"trainable\": False, \"pruning\": None, \"optim\": \"adam\"},\n",
    "    # \"Static_BaseRNN_fewshot\": {\"type\": \"basernn\", \"trainable\": False, \"pruning\": None, \"optim\": \"adam\", \"fewshot\": True},\n",
    "    # \"Static_CNN_RNN_fewshot\": {\"type\": \"cnnrnn\", \"trainable\": False, \"pruning\": None, \"optim\": \"adam\", \"fewshot\": True},\n",
    "  \n",
    "\n",
    "    # Hungarian\n",
    "    # \"Hungarian_DrosoInit_DrosoRef\": {\"type\": \"basernn\", \"trainable\": True, \"pruning\": \"hungarian\", \"optim\": \"adam\", \"init\": \"droso\", \"ref\": \"droso\"},\n",
    "    # # \"Hungarian_DrosoInit_RandSparseRef\": {\"type\": \"basernn\", \"trainable\": True, \"pruning\": \"hungarian\", \"optim\": \"adam\", \"init\": \"droso\", \"ref\": \"randsparse\"},\n",
    "    # # \"Hungarian_DrosoInit_RandStructureRef\": {\"type\": \"basernn\", \"trainable\": True, \"pruning\": \"hungarian\", \"optim\": \"adam\", \"init\": \"droso\", \"ref\": \"randstructure\"},\n",
    "    # # \"Hungarian_RandInit_DrosoRef\": {\"type\": \"basernn\", \"trainable\": True, \"pruning\": \"hungarian\", \"optim\": \"adam\", \"init\": \"random\", \"ref\": \"droso\"},\n",
    "    # \"Hungarian_RandInit_RandSparseRef\": {\"type\": \"basernn\", \"trainable\": True, \"pruning\": \"hungarian\", \"optim\": \"adam\", \"init\": \"random\", \"ref\": \"randsparse\"},\n",
    "    # \"Hungarian_RandInit_RandStructureRef\": {\"type\": \"basernn\", \"trainable\": True, \"pruning\": \"hungarian\", \"optim\": \"adam\", \"init\": \"random\", \"ref\": \"randstructure\"},\n",
    "\n",
    "    # \"Hungarian_RandSparseInit_DrosoRef\": {\"type\": \"basernn\", \"trainable\": True, \"pruning\": \"hungarian\", \"optim\": \"adam\", \"init\": \"randsparse\", \"ref\": \"droso\"},\n",
    "    # \"Hungarian_RandSparseInit_RandSparseRef\": {\"type\": \"basernn\", \"trainable\": True, \"pruning\": \"hungarian\", \"optim\": \"adam\", \"init\": \"randsparse\", \"ref\": \"randsparse\"},\n",
    "    # \"Hungarian_RandSparseInit_RandStructureRef\": {\"type\": \"basernn\", \"trainable\": True, \"pruning\": \"hungarian\", \"optim\": \"adam\", \"init\": \"randsparse\", \"ref\": \"randstructure\"}\n",
    "  \n",
    "    # MLPs\n",
    "    # \"Single_MLP\": {\"type\": \"singlemlp\", \"optim\": \"adam\"},\n",
    "    # \"Twohidden_MLP\": {\"type\": \"twohiddenmlp\", \"optim\": \"adam\"},\n",
    "    # \"Static_MLP\": {\"type\": \"staticmlp\", \"optim\": \"adam\"},\n",
    "    # \"Logistic_Regression\": {\"type\": \"logistic\", \"optim\": \"adam\"},\n",
    "}\n",
    "\n",
    "# 自动生成FewShot版本\n",
    "if fewshot:\n",
    "    fewshot_experiments = {}\n",
    "    for exp_id, config in experiments.items():\n",
    "        # 原始版本\n",
    "        # fewshot_experiments[exp_id] = config\n",
    "        # FewShot版本\n",
    "        fewshot_config = config.copy()\n",
    "        fewshot_config[\"fewshot\"] = True\n",
    "        fewshot_experiments[f\"{exp_id}_fewshot\"] = fewshot_config\n",
    "    experiments = fewshot_experiments\n",
    "\n",
    "\n",
    "def get_input_shape(model_type):\n",
    "    return (1, 1, 28, 28) if model_type in [\"cnnrnn\", \"singlemlp\", \"twohiddenmlp\", \"logistic\"] else (1, 28, 28)\n",
    "\n",
    "def prepare_input(data, model):\n",
    "    return data if isinstance(model, CNNRNN) else data.squeeze(1)\n",
    "\n",
    "def create_fewshot_subset(dataset, epoch_seed):\n",
    "    \"\"\"创建每个epoch的fewshot子集\"\"\"\n",
    "    num_classes = 10\n",
    "    samples_per_class = 120\n",
    "    rng = np.random.default_rng(epoch_seed)\n",
    "    indices = []\n",
    "    targets = np.array(dataset.targets)\n",
    "    for cls in range(num_classes):\n",
    "        cls_indices = np.where(targets == cls)[0]\n",
    "        sampled_indices = rng.choice(cls_indices, samples_per_class, replace=False)\n",
    "        indices.extend(sampled_indices)\n",
    "    return torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer, criterion, train_loader, test_loader, \n",
    "               flops_per_sample, cumulative_flops, cumulative_batches):\n",
    "    \"\"\"训练一个epoch，同时记录两种指标\"\"\"\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    flops_acc_pairs = []\n",
    "    samples_acc_pairs = []\n",
    "\n",
    "    with tqdm(train_loader, unit=\"batch\", desc=\"Training\") as pbar:\n",
    "        for batch_idx, (data, target) in enumerate(pbar):\n",
    "            # ========== 数据准备 ==========\n",
    "            data = prepare_input(data.to(device), model)\n",
    "            target = target.to(device)\n",
    "            batch_size = data.size(0)\n",
    "          \n",
    "            # ========== 计算指标 ==========\n",
    "            cumulative_flops += flops_per_sample * batch_size * 3  # 前向+反向x3\n",
    "            cumulative_batches += 1\n",
    "            current_samples = cumulative_batches // 10\n",
    "\n",
    "            # ========== 训练步骤 ==========\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # ========== 记录指标 ==========\n",
    "            total_loss += loss.item() * batch_size\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += batch_size\n",
    "\n",
    "            # 每10个batch记录一次\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                # 记录FLOPs相关数据（使用训练准确率）\n",
    "                flops_acc_pairs.append((cumulative_flops, correct/total))\n",
    "              \n",
    "                # 按需记录样本数相关数据（使用测试准确率）\n",
    "                if sample:\n",
    "                    test_acc, _ = evaluate(model, test_loader)\n",
    "                    samples_acc_pairs.append((current_samples, test_acc))\n",
    "\n",
    "            # ========== 更新进度条 ==========\n",
    "            pbar_info = {\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'acc': f\"{correct/total:.2%}\",\n",
    "                'FLOPs': f\"{cumulative_flops/1e9:.2f}G\"\n",
    "            }\n",
    "            if sample:\n",
    "                pbar_info['Samples'] = f\"{current_samples}\"\n",
    "            pbar.set_postfix(pbar_info)\n",
    "\n",
    "    # ========== 剪枝操作 ==========\n",
    "    if isinstance(model, BaseRNN) and model.pruning_method == \"hungarian\":\n",
    "        model.apply_hungarian_pruning()\n",
    "    elif isinstance(model, CWSRNN):\n",
    "        model.apply_drosophila_pruning()\n",
    "\n",
    "    return (\n",
    "        total_loss/total, \n",
    "        correct/total, \n",
    "        flops_acc_pairs,\n",
    "        samples_acc_pairs,\n",
    "        cumulative_flops,\n",
    "        cumulative_batches\n",
    "    )\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    activations_list = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = prepare_input(data.to(device), model)\n",
    "            target = target.to(device)\n",
    "          \n",
    "            if isinstance(model, (BaseRNN, CWSRNN)):\n",
    "                batch_size = data.size(0)\n",
    "                r_t = torch.zeros(batch_size, model.hidden_size, device=device)\n",
    "                act_list = []\n",
    "              \n",
    "                E_t = model.input_to_hidden(data.view(batch_size, -1))\n",
    "                W_eff = model.W if isinstance(model, BaseRNN) else (model.C * model.W * model.s.unsqueeze(1))\n",
    "                r_t = torch.relu(r_t @ W_eff + E_t + r_t)\n",
    "                act_list.append(r_t)\n",
    "              \n",
    "                zero_input = torch.zeros(batch_size, model.input_size, device=device)\n",
    "                for _ in range(9):\n",
    "                    E_t = model.input_to_hidden(zero_input)\n",
    "                    r_t = torch.relu(r_t @ W_eff + E_t + r_t)\n",
    "                    act_list.append(r_t)\n",
    "              \n",
    "                batch_activations = torch.stack(act_list, dim=0)\n",
    "                batch_mean_activations = batch_activations.mean(dim=1)\n",
    "                activations_list.append(batch_mean_activations)\n",
    "              \n",
    "                output = model.hidden_to_output(r_t)\n",
    "            else:\n",
    "                output = model(data)\n",
    "          \n",
    "            correct += output.argmax(dim=1).eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "  \n",
    "    if activations_list:\n",
    "        activations = torch.stack(activations_list, dim=0).mean(dim=0).cpu().numpy()\n",
    "    else:\n",
    "        activations = None\n",
    "  \n",
    "    return correct / total, activations\n",
    "\n",
    "def train_experiment(exp_id, config):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    full_train_set = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_set = datasets.MNIST('./data', train=False, transform=transform)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False)\n",
    "\n",
    "    non_zero = None\n",
    "    csv_path = './data/signed_connectivity_matrix.csv' if signed else './data/ad_connectivity_matrix.csv'\n",
    "    if config.get('pruning') == 'drosophila':\n",
    "        W_droso, non_zero = load_drosophila_matrix(csv_path, apply_pruning=True, signed=signed)\n",
    "    else:\n",
    "        W_droso = load_drosophila_matrix(csv_path, apply_pruning=False, signed=signed)\n",
    "\n",
    "    W_init = None\n",
    "    if config.get('init') == 'random':\n",
    "        W_init = torch.randn(W_droso.shape[0], W_droso.shape[0])\n",
    "    elif config.get('init') == 'droso':\n",
    "        W_init = W_droso\n",
    "    elif config.get('init') == 'randsparse':\n",
    "        non_zero_count = np.count_nonzero(W_droso)\n",
    "        total_elements = W_droso.size\n",
    "        mask = torch.zeros(W_droso.shape, dtype=torch.float32)\n",
    "        indices = torch.randperm(total_elements)[:non_zero_count]\n",
    "        mask.view(-1)[indices] = 1\n",
    "        W_init = torch.randn(W_droso.shape) * mask\n",
    "    elif config.get('init') == 'randstructure':\n",
    "        mask = (torch.tensor(W_droso) != 0).float()\n",
    "        W_init = torch.randn(W_droso.shape) * mask\n",
    "    else:\n",
    "        W_init = W_droso if not config.get('trainable', True) else None\n",
    "\n",
    "    W_ref = None\n",
    "    if config.get('ref') == 'droso':\n",
    "        W_ref = W_droso\n",
    "    elif config.get('ref') == 'randsparse':\n",
    "        non_zero_count = np.count_nonzero(W_droso)\n",
    "        total_elements = W_droso.size\n",
    "        mask = torch.zeros(W_droso.shape, dtype=torch.float32)\n",
    "        indices = torch.randperm(total_elements)[:non_zero_count]\n",
    "        mask.view(-1)[indices] = 1\n",
    "        W_ref = torch.randn(W_droso.shape) * mask\n",
    "    elif config.get('ref') == 'randstructure':\n",
    "        mask = (torch.tensor(W_droso) != 0).float()\n",
    "        W_ref = torch.randn(W_droso.shape) * mask\n",
    "\n",
    "    if config['type'] == 'basernn':\n",
    "        model = BaseRNN(\n",
    "            784, W_droso.shape[0], 10,\n",
    "            W_init=W_init,\n",
    "            W_ref=W_ref,\n",
    "            trainable=config['trainable'],\n",
    "            pruning_method=config.get('pruning')\n",
    "        )\n",
    "    if config['type'] == 'drosophilarnn':\n",
    "        model = DrosophilaRNN(\n",
    "            input_dim=784,\n",
    "            sensory_dim=len(conn_data['sensory_ids']),\n",
    "            residual_dim=conn_data['W_rr'].shape[0],\n",
    "            num_classes=10,\n",
    "            conn_weights=conn_data\n",
    "        )\n",
    "    elif config['type'] == 'cwsrnn':\n",
    "        model = CWSRNN(\n",
    "            784, W_droso.shape[0], 10, W_droso,\n",
    "            train_W=config['train_W'],\n",
    "            train_C=config.get('train_C', False),\n",
    "            non_zero_count=non_zero if config.get('pruning') == 'drosophila' else None\n",
    "        )\n",
    "    elif config['type'] == 'cnnrnn':\n",
    "        model = CNNRNN(torch.tensor(W_droso))\n",
    "    elif config['type'] == 'singlemlp':\n",
    "        model = SingleMLP(784, W_droso.shape[0], 10)\n",
    "    elif config['type'] == 'twohiddenmlp':\n",
    "        model = TwohiddenMLP(784, 1360, 10)\n",
    "    elif config['type'] == 'staticmlp':\n",
    "        model = StaticMLP(784, 1360, 10)\n",
    "    elif config['type'] == 'logistic':\n",
    "        model = LogisticRegression(784, 10)\n",
    "    model.to(device)\n",
    "\n",
    "    if config['optim'] == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    else:\n",
    "        optimizer = FISTAOptimizer(model.parameters(), lr=1e-3, lambda_l1=1e-5)\n",
    "\n",
    "    input_shape = get_input_shape(config['type'])\n",
    "    dummy_input = torch.randn(input_shape).to(device)\n",
    "    macs, _ = profile(model, inputs=(dummy_input,))\n",
    "    flops_per_sample = macs * 2 \n",
    "\n",
    "    results = {\n",
    "        \"epoch_train_loss\": [], \"epoch_train_acc\": [],\n",
    "        \"epoch_test_acc\": [], \"flops_acc\": [],\n",
    "        \"samples_acc\": [] if sample else None,  #\n",
    "        \"total_flops\": 0, \"activations\": None\n",
    "    }\n",
    "    cumulative_flops = 0\n",
    "    cumulative_batches = 0 \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 初始评估（epoch 0）\n",
    "    initial_test_acc, initial_activations = evaluate(model, test_loader)\n",
    "    results[\"epoch_test_acc\"].append(initial_test_acc)\n",
    "    results[\"flops_acc\"].append((0, initial_test_acc))\n",
    "    if sample:\n",
    "        results[\"samples_acc\"].append((0, initial_test_acc))  # 样本数从0开始\n",
    "    print(f\"Initial Epoch 0 | Test Acc: {initial_test_acc:.2%}\")\n",
    "\n",
    "    for epoch in range(10):\n",
    "        # 动态创建训练集\n",
    "        if config.get(\"fewshot\"):\n",
    "            train_subset = create_fewshot_subset(full_train_set, epoch_seed=epoch)\n",
    "            train_loader = torch.utils.data.DataLoader(train_subset, batch_size=30, shuffle=True)\n",
    "        else:\n",
    "            train_loader = torch.utils.data.DataLoader(full_train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "        # 训练epoch\n",
    "        epoch_loss, epoch_acc, flops_pairs, samples_pairs, cumulative_flops, cumulative_batches = train_epoch(\n",
    "            model, optimizer, criterion, train_loader, test_loader,\n",
    "            flops_per_sample, cumulative_flops, cumulative_batches\n",
    "        )\n",
    "      \n",
    "        # 记录结果\n",
    "        results[\"epoch_train_loss\"].append(epoch_loss)\n",
    "        results[\"epoch_train_acc\"].append(epoch_acc)\n",
    "        results[\"flops_acc\"].extend(flops_pairs)\n",
    "        if sample:\n",
    "            results[\"samples_acc\"].extend(samples_pairs)\n",
    "      \n",
    "        # Epoch结束后的评估\n",
    "        test_acc, _ = evaluate(model, test_loader)\n",
    "        results[\"epoch_test_acc\"].append(test_acc)\n",
    "        print(f\"Epoch {epoch+1} | Test Acc: {test_acc:.2%}\")\n",
    "\n",
    "    # ========== 保存结果 ==========\n",
    "    filename_suffix = []\n",
    "    if signed: filename_suffix.append(\"signed\")\n",
    "    if sample: filename_suffix.append(\"sample\")\n",
    "    suffix = \".\".join(filename_suffix) if filename_suffix else \"\"\n",
    "    filename = f\"{exp_id}.{suffix}.pkl\" if suffix else f\"{exp_id}.pkl\"\n",
    "  \n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for exp_id, config in experiments.items():\n",
    "        print(f\"\\n{'='*30}\\nTraining {exp_id}\\n{'='*30}\")\n",
    "        train_experiment(exp_id, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Training Single_MLP_fewshot\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m exp_id, config \u001b[38;5;129;01min\u001b[39;00m experiments\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m     \u001b[43mtrain_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 55\u001b[0m, in \u001b[0;36mtrain_experiment\u001b[0;34m(exp_id, config)\u001b[0m\n\u001b[1;32m     50\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# ...（中间保持模型初始化部分不变）...\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# 初始评估（epoch 0）\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m initial_test_acc, initial_activations \u001b[38;5;241m=\u001b[39m evaluate(\u001b[43mmodel\u001b[49m, test_loader)\n\u001b[1;32m     56\u001b[0m results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_train_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_train_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivations\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     62\u001b[0m }\n\u001b[1;32m     63\u001b[0m cumulative_batch_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# 初始化累计batch计数\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, optimizer, criterion, train_loader, test_loader, cumulative_batch_count):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    samples_acc_pairs = []\n",
    "\n",
    "    with tqdm(train_loader, unit=\"batch\", desc=\"Training\") as pbar:\n",
    "        for batch_idx, (data, target) in enumerate(pbar):\n",
    "            data = prepare_input(data.to(device), model)\n",
    "            target = target.to(device)\n",
    "      \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "      \n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += data.size(0)\n",
    "      \n",
    "            # 更新累计的batch计数\n",
    "            cumulative_batch_count += 1\n",
    "            current_samples = cumulative_batch_count // 10\n",
    "      \n",
    "            # 每处理10个batch进行一次评估\n",
    "            if cumulative_batch_count % 10 == 0:\n",
    "                test_acc, _ = evaluate(model, test_loader)\n",
    "                samples_acc_pairs.append((current_samples, test_acc))\n",
    "      \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'acc': f\"{correct/total:.2%}\",\n",
    "                'Samples': f\"{current_samples}\"\n",
    "            })\n",
    "\n",
    "    # 动态剪枝：每个 epoch 结束后执行\n",
    "    if isinstance(model, BaseRNN) and model.pruning_method == \"hungarian\":\n",
    "        model.apply_hungarian_pruning()\n",
    "    elif isinstance(model, CWSRNN):\n",
    "        model.apply_drosophila_pruning()\n",
    "\n",
    "    return total_loss/total, correct/total, samples_acc_pairs, cumulative_batch_count\n",
    "\n",
    "def train_experiment(exp_id, config):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    full_train_set = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_set = datasets.MNIST('./data', train=False, transform=transform)\n",
    "  \n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False)\n",
    "\n",
    "    # ...（中间保持模型初始化部分不变）...\n",
    "\n",
    "    # 初始评估（epoch 0）\n",
    "    initial_test_acc, initial_activations = evaluate(model, test_loader)\n",
    "    results = {\n",
    "        \"epoch_train_loss\": [],\n",
    "        \"epoch_train_acc\": [],\n",
    "        \"epoch_test_acc\": [initial_test_acc],\n",
    "        \"samples_acc\": [(0, initial_test_acc)],  # 初始样本数为0\n",
    "        \"activations\": None\n",
    "    }\n",
    "    cumulative_batch_count = 0  # 初始化累计batch计数\n",
    "    print(f\"Initial Epoch 0 | Test Acc: {initial_test_acc:.2%}\")\n",
    "\n",
    "    for epoch in range(10):\n",
    "        # 动态创建训练集\n",
    "        if config.get(\"fewshot\"):\n",
    "            train_subset = create_fewshot_subset(full_train_set, epoch_seed=epoch)\n",
    "            train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "        else:\n",
    "            train_loader = torch.utils.data.DataLoader(full_train_set, batch_size=64, shuffle=True)\n",
    "    \n",
    "        # 训练过程\n",
    "        epoch_loss, epoch_acc, samples_pairs, cumulative_batch_count = train_epoch(\n",
    "            model, optimizer, criterion, train_loader, test_loader, cumulative_batch_count\n",
    "        )\n",
    "        results[\"samples_acc\"].extend(samples_pairs)\n",
    "        results[\"epoch_train_loss\"].append(epoch_loss)\n",
    "        results[\"epoch_train_acc\"].append(epoch_acc)\n",
    "    \n",
    "        # 每个epoch结束后的测试\n",
    "        test_acc, activations = evaluate(model, test_loader)\n",
    "        results[\"epoch_test_acc\"].append(test_acc)\n",
    "        print(f\"Epoch {epoch+1} | Test Acc: {test_acc:.2%}\")\n",
    "\n",
    "    # 保存结果\n",
    "    filename = f\"{exp_id}.signed.pkl\" if signed else f\"{exp_id}.pkl\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for exp_id, config in experiments.items():\n",
    "        print(f\"\\n{'='*30}\\nTraining {exp_id}\\n{'='*30}\")\n",
    "        train_experiment(exp_id, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_fly_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
