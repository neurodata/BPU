{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def load_connectivity_data(connectivity_path, annotation_path):\n",
    "    df_annot = pd.read_csv(annotation_path)\n",
    "  \n",
    "    mask = (df_annot['celltype'] == 'sensory') & (df_annot['additional_annotations'] == 'visual')\n",
    "    sensory_visual_ids = []\n",
    "    for _, row in df_annot[mask].iterrows():\n",
    "        for col in ['left_id', 'right_id']:\n",
    "            if (id_str := str(row[col]).lower()) != \"no pair\":\n",
    "                sensory_visual_ids.append(int(id_str))\n",
    "\n",
    "    sensory_visual_ids = sorted(list(set(sensory_visual_ids)))\n",
    "    print(f\"Found {len(sensory_visual_ids)} sensory-visual neuron IDs\")\n",
    "  \n",
    "    df_conn = pd.read_csv(connectivity_path, index_col=0)\n",
    "    df_conn.index = df_conn.index.astype(int)\n",
    "    df_conn.columns = df_conn.columns.astype(int)\n",
    "  \n",
    "    valid_sensory_ids = [nid for nid in sensory_visual_ids if nid in df_conn.index]\n",
    "    other_ids = [nid for nid in df_conn.index if nid not in valid_sensory_ids]\n",
    "  \n",
    "    df_reindexed = df_conn.loc[valid_sensory_ids + other_ids, valid_sensory_ids + other_ids]\n",
    "  \n",
    "    adj_matrix = df_reindexed.values * 1e-3  # 统一标准化\n",
    "  \n",
    "    num_S = len(valid_sensory_ids)\n",
    "    return {\n",
    "        'W_ss': adj_matrix[:num_S, :num_S],\n",
    "        'W_sr': adj_matrix[:num_S, num_S:],\n",
    "        'W_rs': adj_matrix[num_S:, :num_S],\n",
    "        'W_rr': adj_matrix[num_S:, num_S:],\n",
    "        'sensory_ids': valid_sensory_ids\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrosophilaRNN(nn.Module):\n",
    "    def __init__(self, input_dim, sensory_dim, residual_dim, num_classes, conn_weights):\n",
    "        super().__init__()\n",
    "        self.W_ss = nn.Parameter(torch.tensor(conn_weights['W_ss'], dtype=torch.float32), requires_grad=True)\n",
    "        self.W_sr = nn.Parameter(torch.tensor(conn_weights['W_sr'], dtype=torch.float32), requires_grad=True)\n",
    "        self.W_rs = nn.Parameter(torch.tensor(conn_weights['W_rs'], dtype=torch.float32), requires_grad=True)\n",
    "        self.W_rr = nn.Parameter(torch.tensor(conn_weights['W_rr'], dtype=torch.float32), requires_grad=True)\n",
    "      \n",
    "        self.input_proj = nn.Linear(input_dim, sensory_dim)\n",
    "        self.output_layer = nn.Linear(residual_dim, num_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "      \n",
    "        assert self.W_ss.shape == (sensory_dim, sensory_dim)\n",
    "        assert self.W_sr.shape == (sensory_dim, residual_dim)\n",
    "        assert self.W_rs.shape == (residual_dim, sensory_dim)\n",
    "        assert self.W_rr.shape == (residual_dim, residual_dim)\n",
    "\n",
    "    def forward(self, x, time_steps=10):\n",
    "        batch_size = x.shape[0]\n",
    "        device = x.device\n",
    "\n",
    "        S = torch.zeros(batch_size, self.W_ss.shape[0], device=device)\n",
    "        R = torch.zeros(batch_size, self.W_rr.shape[0], device=device)\n",
    "      \n",
    "        E = self.input_proj(x)  # [batch_size, sensory_dim]\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            E_t = E if t % 5 == 0 else torch.zeros_like(E)\n",
    "          \n",
    "            S_next = self.activation(\n",
    "                S @ self.W_ss +    # S->S连接\n",
    "                E_t +             # 外部输入\n",
    "                R @ self.W_rs     # R->S连接\n",
    "            )\n",
    "          \n",
    "            R_next = self.activation(\n",
    "                R @ self.W_rr +    # R->R连接\n",
    "                S @ self.W_sr      # S->R连接\n",
    "            )\n",
    "          \n",
    "            S, R = S_next, R_next\n",
    "      \n",
    "        return self.output_layer(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualCNN(nn.Module):\n",
    "    def __init__(self, out_dim=29):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),  # 压缩到 (batch, 32, 1, 1)\n",
    "        )\n",
    "        self.fc = nn.Linear(32, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 1, 28, 28)\n",
    "        z = self.conv(x)           # (batch_size, 32, 1, 1)\n",
    "        z = z.view(z.size(0), -1)  # (batch_size, 32)\n",
    "        return self.fc(z)          # (batch_size, out_dim)\n",
    "\n",
    "# 2) 新的 RNN：无 W_ss，只有 W_sr, W_rs, W_rr\n",
    "class DrosophilaRNNNoWss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        residual_dim,            # 内部脑维度 (R 的大小)\n",
    "        num_classes,\n",
    "        conn_weights,            # 字典，至少包含 W_sr, W_rs, W_rr\n",
    "        sensory_dim=29           # 我们还想让CNN输出 29 维\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 从 conn_weights 中读取 并注册为可学习参数(也可固定不学)\n",
    "        self.W_sr = nn.Parameter(\n",
    "            torch.tensor(conn_weights['W_sr'], dtype=torch.float32), \n",
    "            requires_grad=True\n",
    "        ) # shape (sensory_dim, residual_dim)\n",
    "        self.W_rs = nn.Parameter(\n",
    "            torch.tensor(conn_weights['W_rs'], dtype=torch.float32),\n",
    "            requires_grad=True\n",
    "        ) # shape (residual_dim, sensory_dim)\n",
    "        self.W_rr = nn.Parameter(\n",
    "            torch.tensor(conn_weights['W_rr'], dtype=torch.float32),\n",
    "            requires_grad=True\n",
    "        ) # shape (residual_dim, residual_dim)\n",
    "\n",
    "        # 用 CNN 取代 W_ss\n",
    "        self.visual_cnn = VisualCNN(out_dim=sensory_dim)\n",
    "\n",
    "        self.output_layer = nn.Linear(residual_dim, num_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, time_steps=10):\n",
    "        \"\"\"\n",
    "        x: (batch_size, 1, 28, 28) - MNIST图像\n",
    "        time_steps: 运行多少步RNN迭代\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # 初始化 S, R (如果还需要 S 的初始值，也可以是0或可学习参数)\n",
    "        S = torch.zeros(batch_size, self.W_sr.shape[0], device=device)\n",
    "        R = torch.zeros(batch_size, self.W_rr.shape[0], device=device)\n",
    "\n",
    "        # CNN 只看一次输入，或者每步都看，看需求\n",
    "        # 这里演示：每次t都喂同一张图片\n",
    "        for t in range(time_steps):\n",
    "            # 把图像 CNN 出来得到 29维\n",
    "            E_t = self.visual_cnn(x)  # shape (batch_size, 29)\n",
    "\n",
    "            # 更新方程(去掉 W_ss)\n",
    "            # S_{t+1} = ReLU( CNN输出 + W_sr * R_t )\n",
    "            S_next = self.activation(\n",
    "                E_t + (R @ self.W_sr.transpose(0,1))\n",
    "            )\n",
    "\n",
    "            # R_{t+1} = ReLU( W_rr * R_t + W_rs * S_{t+1} )\n",
    "            R_next = self.activation(\n",
    "                (R @ self.W_rr.transpose(0,1)) + (S_next @ self.W_rs.transpose(0,1))\n",
    "            )\n",
    "\n",
    "            S, R = S_next, R_next\n",
    "\n",
    "        # 最终用内部脑 R 做分类\n",
    "        logits = self.output_layer(R)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29 sensory-visual neuron IDs\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 784]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 89\u001b[0m\n\u001b[1;32m     86\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(results, f)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 53\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 53\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     55\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rnn_fly_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 67\u001b[0m, in \u001b[0;36mDrosophilaRNNNoWss.forward\u001b[0;34m(self, x, time_steps)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# CNN 只看一次输入，或者每步都看，看需求\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# 这里演示：每次t都喂同一张图片\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(time_steps):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# 把图像 CNN 出来得到 29维\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     E_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape (batch_size, 29)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# 更新方程(去掉 W_ss)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# S_{t+1} = ReLU( CNN输出 + W_sr * R_t )\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     S_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\n\u001b[1;32m     72\u001b[0m         E_t \u001b[38;5;241m+\u001b[39m (R \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_sr\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     73\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rnn_fly_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mVisualCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# x: (batch_size, 1, 28, 28)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m           \u001b[38;5;66;03m# (batch_size, 32, 1, 1)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mview(z\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, 32)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(z)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rnn_fly_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rnn_fly_env/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rnn_fly_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rnn_fly_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rnn_fly_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 784]"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    conn_data = load_connectivity_data(\n",
    "        # connectivity_path=\"./data/ad_connectivity_matrix.csv\",\n",
    "        connectivity_path=\"./data/signed_connectivity_matrix.csv\",\n",
    "        annotation_path=\"./data/science.add9330_data_s2.csv\"\n",
    "    )\n",
    "  \n",
    "    model = DrosophilaRNNNoWss(\n",
    "        # input_dim=784,\n",
    "        # sensory_dim=len(conn_data['sensory_ids']),\n",
    "        residual_dim=conn_data['W_rr'].shape[0],\n",
    "        num_classes=10,\n",
    "        conn_weights=conn_data\n",
    "    )\n",
    "    results = {\n",
    "        \"epoch_train_loss\": [],\n",
    "        \"epoch_train_acc\": [],\n",
    "        \"epoch_test_acc\": [],\n",
    "        \"flops_acc\": [],       # 保持结构一致（暂未实现FLOPs计算）\n",
    "        \"total_flops\": 0,      # 保持结构一致\n",
    "        \"activations\": None    # 保持结构一致\n",
    "    }\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "  \n",
    "    train_loader = DataLoader(\n",
    "        datasets.MNIST('./data', train=True, download=True, transform=transform),\n",
    "        batch_size=64, shuffle=True\n",
    "    )\n",
    "  \n",
    "    test_loader = DataLoader(\n",
    "        datasets.MNIST('./data', train=False, transform=transform),\n",
    "        batch_size=64, shuffle=False\n",
    "    )\n",
    "  \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.to(device)\n",
    "  \n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss, correct = 0.0, 0\n",
    "      \n",
    "        for images, labels in train_loader:\n",
    "            images = images.view(-1, 784).to(device)\n",
    "            labels = labels.to(device)\n",
    "          \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          \n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "      \n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "        train_acc = 100. * correct / len(train_loader.dataset)\n",
    "        results[\"epoch_train_loss\"].append(train_loss)\n",
    "        results[\"epoch_train_acc\"].append(train_acc/100)  # 转换为0-1范围\n",
    "      \n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 784).to(device)\n",
    "                labels = labels.to(device)\n",
    "              \n",
    "                outputs = model(images)\n",
    "                test_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "      \n",
    "        test_acc = 100. * test_correct / len(test_loader.dataset)\n",
    "        results[\"epoch_test_acc\"].append(test_acc/100)  # 转换为0-1范围\n",
    "      \n",
    "        print(f\"Epoch {epoch+1}/10:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Test  Acc: {test_acc:.2f}%\")\n",
    "        print(\"-\" * 50)\n",
    "  \n",
    "    with open(\"Drosophila_Metrics.signed.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_fly_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
