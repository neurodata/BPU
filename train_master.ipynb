{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Learnable_RNN_No_Sparsity_fewshot_300 Trial 1 ===\n",
      "========================================\n",
      "Starting Experiment: Learnable_RNN_No_Sparsity_fewshot_300 Trial 1\n",
      "Experiment configuration:\n",
      "  type: basicrnn\n",
      "  trainable: True\n",
      "  init: droso\n",
      "  fewshot: 300\n",
      "  fewshot_batch_size: 17\n",
      "========================================\n",
      "\n",
      "Annotation file: Found 29 sensory neuron IDs\n",
      "Annotation file: Found 400 output neuron IDs\n",
      "Connectivity matrix contains 2952 neurons\n",
      "After filtering, found 29 sensory neurons in matrix\n",
      "After filtering, found 400 output neurons in matrix\n",
      "Remaining 2523 neurons classified as internal\n",
      "BasicRNN init: trainable=True, pruning=None, target_nonzeros=63545, lambda_l1=None\n",
      "LoRA config: use_lora=False, rank=8, alpha=16\n",
      "Trial 1 | Epoch 0 | Test Acc: 9.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 177/177 [00:23<00:00,  7.68batch/s, loss=1.5462, train_acc=18.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submodule nonzero values: {'input_proj': 22765, 'output_layer': 4010, 'activation': 0, 'total': 5573278}\n",
      "Trial 1 | Epoch 1 | Test Acc: 19.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 177/177 [00:22<00:00,  7.80batch/s, loss=1.6001, train_acc=24.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submodule nonzero values: {'input_proj': 22765, 'output_layer': 4010, 'activation': 0, 'total': 5587360}\n",
      "Trial 1 | Epoch 2 | Test Acc: 31.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 177/177 [00:22<00:00,  7.74batch/s, loss=1.1505, train_acc=31.07%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submodule nonzero values: {'input_proj': 22765, 'output_layer': 4010, 'activation': 0, 'total': 5593251}\n",
      "Trial 1 | Epoch 3 | Test Acc: 36.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 177/177 [00:23<00:00,  7.66batch/s, loss=1.6133, train_acc=38.80%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submodule nonzero values: {'input_proj': 22765, 'output_layer': 4010, 'activation': 0, 'total': 5593953}\n",
      "Trial 1 | Epoch 4 | Test Acc: 43.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 177/177 [00:22<00:00,  7.91batch/s, loss=1.1844, train_acc=45.40%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submodule nonzero values: {'input_proj': 22765, 'output_layer': 4010, 'activation': 0, 'total': 5595055}\n",
      "Trial 1 | Epoch 5 | Test Acc: 49.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 177/177 [00:25<00:00,  7.03batch/s, loss=1.1358, train_acc=54.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submodule nonzero values: {'input_proj': 22765, 'output_layer': 4010, 'activation': 0, 'total': 5597388}\n",
      "Trial 1 | Epoch 6 | Test Acc: 52.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|██████▉   | 122/177 [00:16<00:07,  7.68batch/s, loss=0.5592, train_acc=62.34%]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from net import *\n",
    "from connectome_utils import *\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Load config\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "# Global parameters\n",
    "signed = config_data.get(\"signed\", True)\n",
    "sio = config_data.get(\"sio\", True)\n",
    "num_trials = config_data.get(\"num_trials\", 10)\n",
    "num_epochs = config_data.get(\"num_epochs\", 10)\n",
    "batch_size = config_data.get(\"batch_size\", 64)\n",
    "learning_rate = config_data.get(\"learning_rate\", 0.001)\n",
    "experiments = config_data.get(\"experiments\", {})\n",
    "\n",
    "# Few-shot settings\n",
    "fewshot_config = config_data.get(\"fewshot\", {})\n",
    "fewshot_enabled = fewshot_config.get(\"enabled\", False)\n",
    "fewshot_samples = fewshot_config.get(\"samples\", 60)\n",
    "fewshot_batch_size = fewshot_config.get(\"batch_size\", 10)\n",
    "if fewshot_enabled:\n",
    "    fewshot_experiments = {}\n",
    "    for exp_id, exp_config in experiments.items():\n",
    "        cfg = exp_config.copy()\n",
    "        cfg[\"fewshot\"] = fewshot_samples\n",
    "        cfg[\"fewshot_batch_size\"] = fewshot_batch_size\n",
    "        fewshot_experiments[f\"{exp_id}_fewshot_{fewshot_samples}\"] = cfg\n",
    "    experiments = fewshot_experiments\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create stratified few-shot subset\n",
    "def create_fewshot_subset(dataset, seed, samples_per_class=60):\n",
    "    targets = np.array(dataset.targets)\n",
    "    train_size = (samples_per_class * 10) / len(targets)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=train_size, random_state=seed)\n",
    "    indices, _ = next(sss.split(np.zeros_like(targets), targets))\n",
    "    return torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "def get_weight_matrix(base, mode):\n",
    "    if mode == 'random':\n",
    "        # use He Initialization for ReLU\n",
    "        arr_np = (np.random.randn(*base.shape) / np.sqrt(base.shape[0])).astype(np.float32)\n",
    "        return arr_np\n",
    "    \n",
    "    elif mode == 'droso':\n",
    "        return base\n",
    "    \n",
    "    elif mode == 'permuted_droso':\n",
    "        nonzero_vals = base[base != 0].astype(np.float32)\n",
    "        np.random.shuffle(nonzero_vals)\n",
    "        \n",
    "        non_zero_count = len(nonzero_vals)\n",
    "        idx = np.random.choice(base.size, non_zero_count, replace=False)\n",
    "        arr_np = np.zeros_like(base, dtype=np.float32)\n",
    "        \n",
    "        arr_np_flat = arr_np.flatten()\n",
    "        arr_np_flat[idx] = nonzero_vals\n",
    "        arr_np = arr_np_flat.reshape(base.shape)\n",
    "        \n",
    "        return arr_np\n",
    "    elif mode == 'randsparse':\n",
    "        non_zero = np.count_nonzero(base)\n",
    "        mask = np.zeros(base.shape, dtype=np.float32)\n",
    "        idx = np.random.permutation(mask.size)[:non_zero]\n",
    "        mask.flat[idx] = 1\n",
    "        scaling_factor = np.sqrt(non_zero / base.size)  # normalization factor\n",
    "        arr_np = (np.random.randn(*base.shape) * scaling_factor).astype(np.float32) * mask\n",
    "        return arr_np\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_connectivity_info(cfg_data):\n",
    "    if sio:\n",
    "        return load_sio_connectivity_data(\n",
    "            connectivity_path=cfg_data[\"csv_paths\"][\"signed\"],\n",
    "            annotation_path=cfg_data[\"annotation_path\"], rescale_factor=cfg_data.get('rescale_factor', 4e-2)\n",
    "        )\n",
    "    else:\n",
    "        return load_connectivity_data(\n",
    "            connectivity_path=cfg_data[\"csv_paths\"][\"signed\"],\n",
    "            annotation_path=cfg_data[\"annotation_path\"], rescale_factor=cfg_data.get('rescale_factor', 4e-2)\n",
    "        )\n",
    "\n",
    "def load_datasets(transform):\n",
    "    train_set = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_set = datasets.MNIST('./data', train=False, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False)\n",
    "    return train_set, test_loader\n",
    "\n",
    "def initialize_model(config):\n",
    "    if config['type'] == 'basicrnn':\n",
    "        conn = load_connectivity_info(config_data)\n",
    "        # conn['W'] is the rearranged connectivity matrix\n",
    "        W_init = get_weight_matrix(conn['W'], config.get('init'))\n",
    "\n",
    "        # Get LoRA configuration\n",
    "        lora_config = config.get('lora', {})\n",
    "        use_lora = lora_config.get('enabled', False)\n",
    "        lora_rank = lora_config.get('rank', 8)\n",
    "        lora_alpha = lora_config.get('alpha', 16)\n",
    "\n",
    "        return BasicRNN(\n",
    "            W_init=W_init,\n",
    "            input_dim=784,\n",
    "            sensory_dim=conn['W_ss'].shape[0],\n",
    "            internal_dim=conn['W_rr'].shape[0],\n",
    "            output_dim=conn['W_oo'].shape[0],\n",
    "            num_classes=10,\n",
    "            trainable=config.get('trainable'),\n",
    "            pruning=config.get('pruning'),\n",
    "            target_nonzeros=np.count_nonzero(W_init),\n",
    "            lambda_l1=config.get('lambda_l1'),\n",
    "            use_lora=use_lora,\n",
    "            lora_rank=lora_rank,\n",
    "            lora_alpha=lora_alpha\n",
    "        )   \n",
    "    elif config['type'] == 'threehiddenmlp':\n",
    "        return ThreeHiddenMLP(784, 29, 147, 400, 10, config.get('freeze', False))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {config['type']}\")\n",
    "\n",
    "# Train one epoch\n",
    "def train_epoch(model, optimizer, criterion, train_loader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    pbar = tqdm(train_loader, unit=\"batch\", desc=\"Training\")\n",
    "    for data, target in pbar:\n",
    "        data.squeeze(1)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # L1-penalized training-loss to perserve sparsity level\n",
    "        if model.pruning:\n",
    "            logits = model(data)\n",
    "            ce_loss = F.cross_entropy(logits, target)\n",
    "            l1_loss = model.lambda_l1 * model.get_l1_loss() if model.lambda_l1 is not None else 0\n",
    "            loss = ce_loss + l1_loss\n",
    "        else:\n",
    "            loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.size(0)\n",
    "        correct += output.argmax(dim=1).eq(target).sum().item()\n",
    "        total += data.size(0)\n",
    "        train_acc = correct / total if total else 0\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", train_acc=f\"{train_acc:.2%}\")\n",
    "    \n",
    "    if hasattr(model, \"enforce_sparsity\") and model.pruning:\n",
    "        print(\"enforce sparsity start, nonzeros: \", torch.count_nonzero(model.W).item())\n",
    "        model.enforce_sparsity()\n",
    "        print(\"enforce sparsity end, nonzeros: \", torch.count_nonzero(model.W).item())\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "# Evaluate model and compute inference FLOPs\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data.squeeze(1)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            correct += output.argmax(dim=1).eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "    acc = correct / total if total > 0 else 0\n",
    "    return acc\n",
    "\n",
    "# Run training loop and record results\n",
    "def run_training_loop(model, config, full_train_set, test_loader, trial_num, num_epochs, batch_size, fewshot_batch_size):\n",
    "    results = {\"epoch_train_loss\": [],\n",
    "               \"epoch_train_acc\": [],\n",
    "               \"epoch_test_acc\": [],\n",
    "               'submodules_nonzero': [],\n",
    "               'similarity_dict': []}\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # initial evaluation for epoch 0\n",
    "    init_acc = evaluate(model, test_loader)\n",
    "    results[\"epoch_test_acc\"].append(init_acc)\n",
    "\n",
    "    print(f\"Trial {trial_num} | Epoch 0 | Test Acc: {init_acc:.2%}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        if \"fewshot\" in config:\n",
    "            subset = create_fewshot_subset(full_train_set, epoch, config[\"fewshot\"])\n",
    "            train_loader = torch.utils.data.DataLoader(subset, batch_size=config.get(\"fewshot_batch_size\", fewshot_batch_size), shuffle=True)\n",
    "        else:\n",
    "            train_loader = torch.utils.data.DataLoader(full_train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        epoch_loss, epoch_acc = train_epoch(model, optimizer, criterion, train_loader)\n",
    "\n",
    "        results[\"epoch_train_loss\"].append(epoch_loss)\n",
    "        results[\"epoch_train_acc\"].append(epoch_acc)\n",
    "\n",
    "        test_acc = evaluate(model, test_loader)\n",
    "\n",
    "        # save for further flops calculation\n",
    "        submodule_nonzero_dict = {}\n",
    "        for name, submodule in model.named_children():\n",
    "            sub_nonzero = 0\n",
    "            for param in submodule.parameters(recurse=False):\n",
    "                sub_nonzero += torch.count_nonzero(param).item()\n",
    "            submodule_nonzero_dict[name] = sub_nonzero\n",
    "        submodule_nonzero_dict['total'] = sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
    "        results['submodules_nonzero'].append(submodule_nonzero_dict)\n",
    "        results[\"epoch_test_acc\"].append(test_acc)\n",
    "\n",
    "        print(f\"submodule nonzero values: {submodule_nonzero_dict}\")\n",
    "        print(f\"Trial {trial_num} | Epoch {epoch+1} | Test Acc: {test_acc:.2%}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_results(exp_id, config, trial_num, results, signed):\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    filename = f\"{exp_id}_trial{trial_num}\"\n",
    "    if \"fewshot\" in config:\n",
    "        filename = f\"{exp_id}_trial{trial_num}\"\n",
    "    if signed:\n",
    "        filename += \".signed\"\n",
    "    filename += \".pkl\"\n",
    "    with open(os.path.join(\"results\", filename), \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "# Full experiment run\n",
    "def train_experiment(exp_id, config, trial_num):\n",
    "    print(\"========================================\")\n",
    "    print(f\"Starting Experiment: {exp_id} Trial {trial_num}\")\n",
    "    print(\"Experiment configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"========================================\\n\")\n",
    "    torch.manual_seed(trial_num) # TODO\n",
    "    np.random.seed()\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    full_train_set, test_loader = load_datasets(transform)\n",
    "    model = initialize_model(config)\n",
    "    model.to(device)\n",
    "    results = run_training_loop(model, config, full_train_set, test_loader, trial_num,\n",
    "                                  num_epochs, batch_size, fewshot_batch_size)\n",
    "    save_results(exp_id, config, trial_num, results, signed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for exp_id, config in experiments.items():\n",
    "        for trial_num in range(1, num_trials + 1):\n",
    "            print(f\"\\n=== Training {exp_id} Trial {trial_num} ===\")\n",
    "            train_experiment(exp_id, config, trial_num)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_fly_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
